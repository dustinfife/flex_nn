% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nn.R
\name{nn}
\alias{nn}
\title{Fit Neural Network Models Using Formula Syntax}
\usage{
nn(
  formula,
  data,
  epochs = 100,
  batch_size = 4,
  validation_split = 0.2,
  layers = NULL,
  optimizer = optimizer_adam(),
  loss = "mse",
  metrics = list("mean_absolute_error"),
  verbose = 0,
  ...
)
}
\arguments{
\item{formula}{A formula specifying the model (e.g., y ~ x1 + x2 + x3)}

\item{data}{A data frame containing the variables specified in the formula}

\item{epochs}{Integer. Number of training epochs (default: 100)}

\item{batch_size}{Integer. Batch size for training (default: 4)}

\item{validation_split}{Numeric. Fraction of data to use for validation (default: 0.2)}

\item{layers}{Function or NULL. A function that takes (model, input_dim) and returns
the model with layers added. If NULL, uses a default two-hidden-layer architecture}

\item{optimizer}{Keras optimizer object. Default is optimizer_adam()}

\item{loss}{Character string or Keras loss function. Default is "mse"}

\item{metrics}{List of metrics to track during training. Default is "mean_absolute_error"}

\item{verbose}{Integer. Verbosity level (0 = silent, 1 = progress bar, 2 = one line per epoch)}

\item{...}{Additional arguments (reserved for future use)}
}
\value{
A list containing:
  \item{model}{The trained Keras model}
  \item{history}{Training history object}
  \item{x_means}{Column means used for normalization}
  \item{x_sds}{Column standard deviations used for normalization}
  \item{formula}{The original formula}
  \item{x}{The original predictor matrix (before scaling)}
  \item{y}{The original response vector}
  \item{var_names}{Names of predictor variables}
}
\description{
This function provides a formula-based interface for training neural networks
using Keras/TensorFlow. It handles data preprocessing, normalization, and 
provides flexible layer specification while maintaining compatibility with
flexplot's compare.fits function.
}
\details{
The function automatically:
\itemize{
  \item Extracts predictors and response from the formula
  \item Handles factor variables through model.matrix expansion
  \item Normalizes predictors using z-score standardization
  \item Stores normalization parameters for later prediction
  \item Creates a sequential Keras model with specified architecture
}

The default layer architecture consists of:
\itemize{
  \item Input layer with 8 units and ReLU activation
  \item Hidden layer with 4 units and ReLU activation  
  \item Output layer with 1 unit (linear activation)
}

For custom architectures, provide a layers function that takes the model object
and input dimension, then adds layers and returns the modified model.
}
\examples{
\dontrun{
# Load required libraries
library(keras)
library(tensorflow)

# Basic usage with default architecture
data(mtcars)
model1 = nn(mpg ~ hp + wt + cyl, data = mtcars, epochs = 50)

# Custom layer architecture with dropout
custom_layers = function(model, input_dim) {
  model \%>\%
    layer_dense(units = 64, activation = "relu", input_shape = input_dim) \%>\%
    layer_dropout(rate = 0.3) \%>\%
    layer_dense(units = 32, activation = "relu") \%>\%
    layer_dropout(rate = 0.2) \%>\%
    layer_dense(units = 16, activation = "relu") \%>\%
    layer_dense(units = 1)
}

model2 = nn(mpg ~ hp + wt + cyl, data = mtcars, 
            layers = custom_layers, epochs = 100)

# Deep network example
deep_layers = function(model, input_dim) {
  model \%>\%
    layer_dense(units = 128, activation = "relu", input_shape = input_dim) \%>\%
    layer_batch_normalization() \%>\%
    layer_dense(units = 64, activation = "relu") \%>\%
    layer_batch_normalization() \%>\%
    layer_dense(units = 32, activation = "relu") \%>\%
    layer_dense(units = 16, activation = "relu") \%>\%
    layer_dense(units = 1)
}

model3 = nn(mpg ~ ., data = mtcars, layers = deep_layers, 
            epochs = 200, batch_size = 8)

# Classification example (binary)
classification_layers = function(model, input_dim) {
  model \%>\%
    layer_dense(units = 32, activation = "relu", input_shape = input_dim) \%>\%
    layer_dense(units = 16, activation = "relu") \%>\%
    layer_dense(units = 1, activation = "sigmoid")
}

# Convert to binary outcome
mtcars$high_mpg = as.numeric(mtcars$mpg > median(mtcars$mpg))
model4 = nn(high_mpg ~ hp + wt + cyl, data = mtcars,
            layers = classification_layers,
            loss = "binary_crossentropy",
            metrics = list("accuracy"))

# Use with flexplot's compare.fits
library(flexplot)
lm_model = lm(mpg ~ hp + wt + cyl, data = mtcars)
compare.fits(mpg ~ hp | wt, data = mtcars, lm_model, model1$model)
}

}
\seealso{
\code{\link[keras]{keras_model_sequential}}, \code{\link[flexplot]{compare.fits}}
}
\author{
Dustin Fife
}
